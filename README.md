## Background

- Auto Regressive
- Auto Encoding

## Before Pre-trained

- Word2Vec
- Seq2Seq

## After Pre-trained

- Attention Machanism
  - 고정된 벡터에 정보 압축시 정보 손실 발생
  - RNN: Vanishing Gradient
  - [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf))
- Seq2Seq + Attention
- Transformer
  - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- GPT-1
  - **[OpenAI GPT-1 - Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)**
- BERT
  - [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- XLNet
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- RoBERTa
  - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- ALBERT
  - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- T5
  - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- ELECTRA
  - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
- GPT3
  - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)



